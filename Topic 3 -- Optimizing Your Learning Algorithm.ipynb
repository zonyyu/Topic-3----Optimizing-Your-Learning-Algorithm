{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74cd41cd",
   "metadata": {},
   "source": [
    "# Topic 3 -- Optimizing Your Learning Algorithm\n",
    "\n",
    "Welcome back to the third topic of the Beginner AI Course! Now that we've come this far, together we have trained a handful of Linear and Logistic Regression algorithms. So far, machine learning seems quite plug-and-play. However, knowing how to put together a learning algorithm and train is only a **small part of ML**. A hugely important aspect of ML is **knowing how to optimize your learning algorithm**. In fact, this is what often separates amateur ML practitioners and advanced ML practitioners. This notebook covers bias and variance analysis, regularization, and lots more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac255653",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Importing Modules](#mods)\n",
    "    - [Feature Scaling](#scale)\n",
    "    - [Standard Norm Implementation](#stdnorm)\n",
    "\n",
    "\n",
    "2. [Bias and Variance](#bav)\n",
    "    - [Loading our Dataset](#loading)\n",
    "    - [Training Models with Different Degrees](#deg)\n",
    "    - [Overfitting and Underfitting](#over)\n",
    "    - [Observations](#obs1)\n",
    "    \n",
    "    \n",
    "3. [Cross Validation](#crossval)\n",
    "    - [Secondary Metric](#metric)\n",
    "    - [Why do we even need Cross Validation?](#why)\n",
    "    - [Summary](#summary)\n",
    "    \n",
    "    \n",
    "4. [How to Deal with High Bias and High Variance](#deal)\n",
    "     - [Fixing the problem of Underfitting](#under)\n",
    "     - [Fixing the problem of Overfitting](#over)\n",
    "     \n",
    "\n",
    "5. [Regularization](#reg)\n",
    "    - [L2 Regularization](#l2)\n",
    "    - [L1 Regularization](#l1)\n",
    "    - [L2 Regluarization in Action](#action)\n",
    "    - [Observations](#obs2)\n",
    "    \n",
    "    \n",
    "6. [The ML Process](#process)\n",
    "    - [Iterate, Iterate, Iterate!](#iterate)\n",
    "    - [Suggestions on Training Your ML Model](#suggestions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7493f6",
   "metadata": {},
   "source": [
    "## Importing Modules <a name=\"mods\">\n",
    "\n",
    "First things first, lets import our modules. Here is a brief description of the modules:\n",
    "- **Numpy**: powerful linear algebra library\n",
    "- **Pandas**: data organization and visualization \n",
    "- **SKLearn**: machine learning library with many data-science tools\n",
    "- **Bokeh**, **SeaBorn**, and **MatPlotLib**: data plotting and visualization libraries\n",
    "- **utils**: A custom module to display graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1eef0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d41d5",
   "metadata": {},
   "source": [
    "## Feature Scaling <a name=\"scale\">\n",
    "\n",
    "Very often in machine learning, we find a dataset where the values seem to be on completely different **orders of magnitude**. For example, going back to the car mileage dataset, the **mileage** can range from 0 to 500,000 km, while the engine capacity ranges from 1.0 to 3.5. The massive difference in input space can cause some serious problems when trying to fit our models.\n",
    "\n",
    "A solution to this problem is to **normalize** our data. This **scales** all the input values so that they are on a similar order of magnitude. Without getting into the mathematical details, here are some illustrations of cost functions of **non-normalized** and **normalized** inputs:\n",
    "\n",
    "<img src=\"images/norm.png\" alt=\"cannot display image\" width=700>\n",
    "\n",
    "As you can see, **normalizing** the input data results in a **circular cost function** compared to the **elongated cost function** that comes from **non-normalized** data. This results in much faster gradient descent as well as greater stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567a09f",
   "metadata": {},
   "source": [
    "### Standard Norm Implementation <a name=\"stdnorm\"></a>\n",
    "\n",
    "**Standard Normalization** is the most popular method of feature scaling used to make the cost function **easier to traverse**. While how it works is buried behind various math and statistics concepts we are not going to cover, standard norm can often make the difference between being able to train a learning algorithm and not being able to train it at all. In fact, the PyTorch `BatchNormalization` module was used for the purpose of standard normalization in previous programming projects, because without it, your models would have taken **hours** to train!\n",
    "    \n",
    "    \n",
    "To implement standard norm in SKLearn, simply set the `normalize` parameter to True:\n",
    "    \n",
    "```python\n",
    "model = LinearRegression(normalize=True) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f241213",
   "metadata": {},
   "source": [
    "## Bias and Variance <a name=\"bav\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ab1e7",
   "metadata": {},
   "source": [
    "Previously, we have seen Linear Regression at work on the `cars.csv` dataset. We noted that extracting **polynomial features** from our dataset greatly increased the flexibility of our model, and thus led to a higher $R^2$ score than regular multi-variate linear regression. Does that mean *if we increase the degree of our polynomial, we will achieve higher correlation?*. Lets see for ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad3054",
   "metadata": {},
   "source": [
    "### Loading our Dataset <a name=\"loading\">\n",
    "\n",
    "This is the same dataset used in Topic 1. This dataset include the features, mileage, engine type, price, and many more about cars. We will start off by reading the `csv` file into a **Pandas DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f317418",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44e8f8",
   "metadata": {},
   "source": [
    "To start, we are going to use the exact same features as before, notably the `odometer_value`, `year_produced`, `engine_capacity`, `drivetrain`. Note that we have to convert `drivetrain` to categorical values. While we are at it, we can shuffle the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b594d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de42ec",
   "metadata": {},
   "source": [
    "### Training Models with Different Degrees <a name=\"deg\">\n",
    " \n",
    "Now that we have our **features** and **labels** defined, we can train our models! We are going to train three models with polynomial degrees of **1, 3,** and **10**. Keep in mind a polynomial of degree 1 is equivalent to regular multi-variate linear regression. Remember, our goal is to test **whether or not fitting higher degree polynomials would result in a better performing model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c340d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce36aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8d0b7",
   "metadata": {},
   "source": [
    "Yikes... It seems that our degree-10 polynomial regression performs the least consistent, many times with $R^2$ scores in the negatives. Now the question becomes, *Why does a high degree polynomial perform so poorly?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a2187",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting <a name=\"over\">\n",
    "\n",
    "When dealing with large-scale machine learning projects, **very rarely** do we ever train the perfect model in the first try. Often times we run into fitting issues where our model is not fitting as well as we hope. Let's first remind ourselves about a few concepts we learned previously before we look at some visuals:\n",
    "\n",
    "1. Gradient Descent will try to find the parameters that **minimize** the cost function. \n",
    "2. We use a **testing set** to evaluate our model, because these are data the model has never seen from training, and thus they cannot simply \"memorize\" the training set and apply on testing data. \n",
    "\n",
    "With that said, lets take a look at our first fitting problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6653ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'disp_underfit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-56942eb4a4f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisp_underfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'disp_underfit' is not defined"
     ]
    }
   ],
   "source": [
    "disp_underfit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d808d29",
   "metadata": {},
   "source": [
    "This model is said to have **\"Underfit\"** the data. This is because the the model is **too simple**, and is unable to fit the general trend of the data. When a model is underfitting, we say that the model has **High Bias**. Taking a look back at the three models we trained, `model1` underfits the data because for this specific problem of car prices, using a linear relation (polynomial degree of 1) simply isn't enough for the model to fit the data. With underfitting models, **training error is high and testing error is high**.\n",
    "\n",
    "Lets take a look at our next example of poor fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b0cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_overfit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb8b34",
   "metadata": {},
   "source": [
    "This model is said to have **\"Overfit\"** the data. The model is **far too complex**, (In this case it is a degree 10 polynomial), and fits the training set way too well. When the model trains, gradient descent **minimizes** the cost function. Here, the hypothesis function seems to **pass through every single point**, so the cost is **virtually zero**. However, when we expose our model to testing data, it will **fit the testing data very poorly**. Overfitting models are said to have **High Variance**. With overfitting models, **training error is low but testing error is high**\n",
    "\n",
    "Finally, let's see a properly-fit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_good_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b9ca3",
   "metadata": {},
   "source": [
    "Notice that with a properly-fit model, there may still be a bit of errors with the training set and the hypothesis function, but that is okay, as this is still the model that **generalizes** the best onto the test set. In this example, we used a degree 3 polynomial. When a model is properly fit, **both training error and testing error are low**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fcf0bc",
   "metadata": {},
   "source": [
    "### Observations <a name=\"obs1\">\n",
    "    \n",
    "Before we forget, it's important to document our findings. We noted that `model1` was too simple and underfit our training data, `model3` had the best fit out of all the models, and `model10` severely overfit our data. Let's write this down.\n",
    "    \n",
    "| Model | Observations | R2 |\n",
    "|:--- | :--- | :--- |\n",
    "| model1 | Mediocre fit on our testing set | 60% |\n",
    "| model3 | Good fit of our testing set | 80% |\n",
    "| model10 | Inconsistant fit to our testing set, most likely due to overfitting | NA |\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690b043",
   "metadata": {},
   "source": [
    "## Cross Validation <a name=\"crossval\">\n",
    "    \n",
    "We've seen how **high variance** (overfitting) and **high bias** (underfitting) looks like in terms of how the hypothesis function looks like when fitted to the data, however in most Machine Learning problems, we simply **cannot visualize the hypothesis function**. However, when training, the **cost function** can give us a clue to how our model is fitting.\n",
    "    \n",
    "Recall that we just talked about the differences in training error and testing error between high variance, high bias, and correctly fit models. The goal is to visualize the **trends** in training and testing error over the **many iterations** we train our model. For this, **we will create a new split in our data**. Up until now, we split our dataset into training sets and testing sets. Now, we will create a new split called **cross validation**. This set is very similar to the testing set, as the **model will not be learning from the cross validation set**.\n",
    "    \n",
    "<img src=\"images/train_val_test.png\" alt=\"cannot display image\" width=\"700px\">\n",
    "    \n",
    "Now, after every training iteration of our model, we will record the **training error** as well as the **cross validation** error. These error plots can give us clues on whether our model is overfitting, underfitting, or correctly fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_cost_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e6bc1",
   "metadata": {},
   "source": [
    "**High Bias**: \n",
    "- High training costs, suggesting that your model cannot even fit the training set, let alone the cross validation and test sets.\n",
    "- Even higher cross validation cost\n",
    "\n",
    "**High Variance**:\n",
    "- Low training costs sugessting a very good fit to the training set\n",
    "- High cross validation costs sugesting a very poor fit to the cross validation set\n",
    "\n",
    "**Proper fit**:\n",
    "- Low training costs sugesting a very good fit to the training set\n",
    "- Low cross validation costs sugesting a good fit to the cross validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aaad0e",
   "metadata": {},
   "source": [
    "### Secondary Metric <a name=\"metric\"></a>\n",
    "    \n",
    "Our primary metric has been our losses, both for **training** and **cross validation**. While this gives us a good way of determining whether our model is overfitting or underfitting, sometimes its good to also carry a **secondary metric** for further validation. \n",
    "    \n",
    "Recall our **GPU Performance Predictor**: We trained a polynomial regression model that had **massive training losses**. We might conclude that the model was underfitting, however we saw that the $R^2$ score was around 0.95, which shows that our model is actually fitting very nicely. Thus, whether it is linear regression or logistic regression, it's good to include **$R^2$ scores** or **Accuracy and F1** alongside the loss.\n",
    "\n",
    "### Why do we even need Cross Validation <a name=\"why\"></a>\n",
    "\n",
    "So far we've looked at how cross validation could be used to monitor the model's **costs every iteration** on data it's never seen before. That begs the question: *Is the cross validation set even necessary? Why can't we use the test set as the cross validation set? Also, why can't we use the cross validation set as the test set?* \n",
    "\n",
    "In machine learning, often times your model requires so much data that you have to collect data from multiple **sources**. Say you are creating a **cat-classifier**. You decided to collect **labeled training data** from online, and for the testing set, you took pictures of cats with your phone. Notice that this scenario is **quite common**, where you have a **labeled dataset you use for training**, and the **test set contains no labels**. This is because labelling data is very **time consuming**, so people often skip labelling the test set.\n",
    "\n",
    "So far here's the run-down: You have a labeled training set obtained from online, and an un-labeled test set obtained from your phone camera. Unfortunately, you cannot use the test set to moniter the per-iteration loss, because there is **no label to compute the loss!**\n",
    "\n",
    "You want to compute the loss per iteration so you can tune your model, so you split off some examples from your labeled training set and use it for **cross-validation**. By using your newly-formed cross validation set, you were able to create **cost graphs** and prevent your model from overfitting. \n",
    "\n",
    "#### Question for students:\n",
    "Now your model no longer overfits to the training set, and generalizes to the cross validation set. Is this enough?\n",
    "\n",
    "> #### Anser:\n",
    "> **No!** While your model performs **very well on the cross validation set**, it **might not perform that well on the test set!** This is because your test set comes from your **phone**, whereas when you tuned the model to work well on the cross validation set, you are tuning your model to work well on photos **from the internet**. These are two **different** sources, therefore it is necessary to **check with your test set** to determine whether or not it also generalizes here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aec1c2",
   "metadata": {},
   "source": [
    "### Summary <a name=\"summary\">\n",
    "    \n",
    "Creating a cross validation split of your data allows you to **track** the **costs** and a **secondary metric** of your learning algorithm **on a set of data it has never seen before**, thus allowing you to catagorize your algorithm as **underfitting, overfitting,** or **fitting just right**. While the cross validation set behaves similarly to the test set, it should not replace the test set.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4baac4e",
   "metadata": {},
   "source": [
    "## How to Deal with High Bias and High Variance <a name=\"deal\">\n",
    "    \n",
    "Now that we've demonstrated how to **diagnose** whether your model was overfitting or underfitting, now we need to talk about how to **solve** overfitting and underfitting problems. \n",
    "    \n",
    "### Fixing the problem of Underfitting <a name=\"under\">\n",
    "    \n",
    "Luckily, underfitting is one of the **easiest** problems to solve. Just get a bigger model! That can be done by obtaining more features to train on (**Hint: One way to do that is to create polynomial features, which we have been doing for the past little bit.**)\n",
    "    \n",
    "### Fixing the problem of Overfitting <a name=\"over\">\n",
    "    \n",
    "Overfitting can be **much harder** to fix. It happens when your model is **too complicated**, and is able to fit the training data in ways that are undesireable. This is why in supervised learning, there are many methods of preventing overfitting, including **Dropout**, **Batch Norm**, etc. In this chapter we are going to talk about **Regularization**.\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93380118",
   "metadata": {},
   "source": [
    "## Regularization <a name=\"reg\">\n",
    "\n",
    "Regularization is a powerful method of **reducing the complexity** of a learning algorithm. This is very helpful in preventing your model from **overfitting**. Today, we are going to talk about **two** of the most popular regularization methods.\n",
    "\n",
    "### L2 Regularization <a name=\"l2\">\n",
    "    \n",
    "L2 Regularization, also known as **Ridge** regularization, is by far the most popular method of regularization, therefore we will use this throughout this course. L2 works by **preventing** the weights and bias from getting **too large**, therefore keeping the model from becoming too complex. Let's take a look at what happens when we apply regularization to our degree 10 Polynomial. For this example, we found that a regularization parameter of 3.36 works the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bcf395",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_reg(3.36)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b706bf",
   "metadata": {},
   "source": [
    "### L1 Regularization <a name=\"l1\">\n",
    "    \n",
    "L1 Regularization, also known as **Lasso** regularization, is not as popular as L2, but achieves the same goal of keeping weights and bias from getting too large. We won't get into the details of L1 regularization in this course.\n",
    "    \n",
    "    \n",
    "### L2 Regularization in Action <a name=\"action\">\n",
    "    \n",
    "Previously, we've trained three polynomial regression models with degrees of **1, 3,** and **10**. We noticed that `model1` underfits the data, therefore there is not much regularization would do to fix the problem. However, `model10` overfits the data, therefore lets see if Ridge regularization could **simplify** the model so that it fits better.\n",
    "    \n",
    "Here, we will use the `Ridge()` class to train our new regularized model. The `Ridge` class takes in a parameter `alpha` that represents the 'regularization strength' -- 0 means no regularization, and higher values will have stronger regularizations. Try `alpha = 1e-8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37db468",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c2b98",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "> Try using different values of `alpha`, see if you can improve the $R^2$ score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c4e49",
   "metadata": {},
   "source": [
    "### Observations <a name=\"obs2\">\n",
    "\n",
    "The difference regularization makes is tremendous. The model went from an overfitting mess into one that performs either **on par or better than our best performing model which fits a degree 3 polynomial**. Let's record our findings down below.\n",
    "    \n",
    "| Model | Observations | R2 |\n",
    "|:--- | :--- | :--- |\n",
    "| model1 | Mediocre fit on our testing set | 60% |\n",
    "| model3 | Good fit of our testing set | 80% |\n",
    "| model10 | Inconsistant fit to our testing set, most likely due to overfitting | NA |\n",
    "| model_ridge | Much better fit to our testing set, despite using a degree 10 polynomial | 81% |\n",
    "    \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83c981",
   "metadata": {},
   "source": [
    "## The ML Process <a name=\"process\">\n",
    "    \n",
    "Congrats on making it this far! In the previous topics, we've built and played around with Linear Regression and Logistic Regression models. As far as the programming projects go, we gave you the ideal **learning rates** and **epochs**, as well as set up some other parameters so that our model would perform well. However, Machine Learning in truth is quite different, as you'll need to experiment to find the perfect **hyperparameters** for training and evaluating. \n",
    "    \n",
    "    \n",
    "### Iterate, Iterate, Iterate! <a name=\"iterate\">\n",
    "    \n",
    "Machine Learning is a highly **iterative** process. Sometimes training a model can take **weeks, months,** or even **years** to do. It is **completely normal** to train a learning algorithm and not get it right the first time. Here is a graphic created by **Dr. Andrew Ng** of Stanford University, one of the leading researchers of deep learning.\n",
    "    \n",
    "<img src=\"images/machine-learning-iterative-process.png\" alt=\"Cannot display image\" width=300px>\n",
    "    \n",
    "1. **Idea**: Start off with an idea of what you want to do, whether it is create a new ML algorithm or tune your existing algorithm\n",
    "    \n",
    "2. **Code**: Carry out your idea in code\n",
    "    \n",
    "3. **Experiment**: Test your code to see if your idea worked or not. Often times the results of experimentation will bring up new ideas for you to implement.\n",
    "    \n",
    "    \n",
    "#### Hot Take:\n",
    "The faster you can iterate, the sooner you can deploy your machine learning model. **HOWEVER**: fast iteration does not equal rushed iteration.\n",
    "    \n",
    "    \n",
    "    \n",
    "### Suggestions on Training Your ML Model <a name=\"suggestions\">\n",
    "    \n",
    "1. **Make sure your error is going down**: If your loss ever becomes `NaN` or gets bigger, that means your learning rate is too high.\n",
    "    \n",
    "    \n",
    "2. If you think your model is **underfitting**, go for a bigger model (get more features, use polynomial features)\n",
    "    \n",
    "    \n",
    "3. **Make sure your model fits the training set well enough first**: During development, the first few trials should overfit. **If it can't even fit the training set, it cannot fit the cross validation and testing sets!**\n",
    "    \n",
    "    \n",
    "4. **AFTER** you determined that your model can fit the training set (overfitting), **THEN** apply regularization to make it generalize to the cross validation set\n",
    "    \n",
    "    \n",
    "5. **Keep Iterating** until you are satisfied with the cross validation loss as well as the cross validation accuracy or $R^2$. On every iteration, **tune** your hyperparameters such as **learning rate** and **regularization**, and compare with the previous iteration to see if your model is improving\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd89b6f",
   "metadata": {},
   "source": [
    "## $\\mathcal{Fin}$ <a name=\"fin\">\n",
    "    \n",
    "Congrats on making it to the end of this notebook! Next up, we will explore the power of regularization in a made-up storyline titled **\"Cat-astrophe\"**.\n",
    "    \n",
    "<img src=\"images/programmer.jpg\" alt=\"Cannot display image\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf73d52",
   "metadata": {},
   "source": [
    "## References\n",
    "- [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "- [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "- [https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)\n",
    "\n",
    "- [https://towardsdatascience.com/why-you-failed-your-machine-learning-interview-5ebb02b9f69c](https://towardsdatascience.com/why-you-failed-your-machine-learning-interview-5ebb02b9f69c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7395ab4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
